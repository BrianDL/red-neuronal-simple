\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\usepackage{listings}
\usepackage{xcolor}

% Define a custom listing style
\lstdefinestyle{customcode}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    tabsize=2,
    showstringspaces=false,
    linewidth=0.98\columnwidth,  % Set width to 90% of the column width
    xleftmargin=0.02\columnwidth,  % Add left margin
    xrightmargin=0.00\columnwidth  % Add right margin
}



\title{Implementando una Red Neuronal Desde Cero}
\author{Brian David Leiva - ECFM, USAC.}
\date{Noviembre, 2024}

\begin{document}

\maketitle

\begin{abstract}
Describimos la implementación de una red neuronal simple desde cero utilizando el lenguaje de programación Zig. Se detallan los componentes fundamentales de la red, incluyendo la estructura de datos para neuronas y capas, la propagación hacia adelante, y un método de entrenamiento simplificado. El proyecto demuestra la capacidad de la red para aprender una función lineal simple.
\end{abstract}

\section{Introducción}
Las redes neuronales son modelos computacionales inspirados en el funcionamiento de las neuronas biológicas. Estas estructuras de procesamiento de información están diseñadas para reconocer patrones, aprender de los datos y realizar tareas complejas como clasificación, regresión y toma de decisiones.

Una red neuronal típica consta de múltiples capas de unidades de procesamiento interconectadas llamadas neuronas artificiales. Cada neurona recibe entradas, las procesa y produce una salida.

El funcionamiento de una sola neurona se puede describir en dos pasos principales. Primero, la neurona calcula la entrada ponderada. Cada neurona recibe múltiples entradas ($x_1, x_2, ..., x_n$), y cada entrada está asociada con un peso correspondiente ($w_1, w_2, ..., w_n$). La neurona calcula la suma ponderada de estas entradas, también conocida como "suma ponderada" o "weighted sum":

\[
z = \sum_{i=1}^n w_i x_i + b
\]

Donde $z$ es la suma ponderada, $w_i$ son los pesos, $x_i$ son las entradas, $n$ es el número de entradas, y $b$ es el sesgo (bias) de la neurona, que permite ajustar el umbral de activación. Esta expresión matemática representa cómo la neurona combina sus entradas para producir un valor único.

En el segundo paso, la suma ponderada $z$ se pasa a través de una función de activación $f$, que introduce no linealidad en el modelo: 

\[y = f(z)\]

donde $y$ es la salida final de la neurona. La función de activación $f$ puede tomar varias formas, dependiendo de los requisitos específicos del problema. Algunas funciones de activación comunes incluyen la función sigmoide ($f(z) = \frac{1}{1 + e^{-z}}$), la tangente hiperbólica (tanh, $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$), y la ReLU (Rectified Linear Unit, $f(z) = \max(0, z)$).

Este proceso de combinación de entradas ponderadas seguido por una función de activación permite a la neurona aprender y representar relaciones complejas en los datos. Cuando se conectan múltiples neuronas en capas, formando una red neuronal, esta capacidad se amplifica, permitiendo al modelo capturar patrones y relaciones aún más complejas en los datos de entrada.

La estructura básica de una red neuronal incluye una capa de entrada que recibe los datos iniciales, capas ocultas que procesan la información extrayendo y transformando características, y una capa de salida que produce el resultado final de la red. El funcionamiento de una red neuronal se basa en dos procesos principales: la propagación hacia adelante, donde los datos fluyen desde la capa de entrada, a través de las capas ocultas, hasta la capa de salida; y la retropropagación y aprendizaje, donde la red ajusta sus pesos internos para minimizar la diferencia entre sus predicciones y los resultados deseados.


En este proyecto tenemos como objetivo implementar la red neuronal más simple posible desde cero, utilizando el lenguaje de programación Zig. Nos enfocaremos en construir los componentes fundamentales: neuronas, capas, y los mecanismos de propagación hacia adelante y entrenamiento básico. Esta implementación nos permitirá comprender en profundidad cómo funcionan las redes neuronales a nivel fundamental.

Nuestra implementación será básica pero sentará las bases para entender conceptos más avanzados en el campo del aprendizaje profundo, como redes más complejas, algoritmos de optimización sofisticados y arquitecturas especializadas para tareas específicas.

\section{Metodología}
\subsection*{Lenguaje de Programación}
Elegimos Zig como lenguaje de programación para este proyecto debido a su enfoque en el rendimiento, la seguridad de la memoria y la simplicidad. Zig proporciona un control de bajo nivel similar a C, pero con características modernas que facilitan el desarrollo de sistemas complejos.

\subsection*{Implementación}

\subsubsection*{Estructura de Neurona}
Se implementó una estructura \texttt{Neurona} que contiene pesos, sesgo y un allocator para la gestión de memoria:

\begin{lstlisting}[style=customcode]
const Neurona = struct {
    pesos: []f32,
    sesgo: f32,
    allocator: *std.mem.Allocator,
};
\end{lstlisting}

\subsubsection*{Estructura de Capa}
La estructura \texttt{Capa} representa una colección de neuronas y define la función de activación para todas las neuronas en esa capa:

\begin{lstlisting}[style=customcode]
const Capa = struct {
    neuronas: []Neurona,
    funcion_activacion: fn (f32) f32,
    allocator: *std.mem.Allocator,
};
\end{lstlisting}

Es importante notar que la función de activación se define a nivel de capa, no a nivel de neurona individual. Esta decisión de diseño se alinea con la práctica común en el campo de las redes neuronales, donde es habitual referirse a una capa por la función de activación que utiliza (por ejemplo, "capa ReLU" o "capa sigmoide"). Esta aproximación ofrece varias ventajas: simplifica la definición y manipulación de la arquitectura de la red, permite optimizaciones al aplicar la misma función de activación a todas las neuronas de una capa, y refleja cómo se conciben y discuten las arquitecturas de redes neuronales en la literatura y la práctica. Además, esta estructura de capa nos proporciona una gran flexibilidad en el diseño de la red, permitiendo utilizar diferentes funciones de activación para diferentes capas si así lo requerimos, lo cual es necesario para crear arquitecturas de redes neuronales más complejas y especializadas.


\subsubsection*{Estructura de Red Neuronal}
La estructura \texttt{RedNeuronal} representa la red completa, compuesta por capas:

\begin{lstlisting}[style=customcode]
pub const RedNeuronal = struct {
    capas: []Capa,
    strat_inicia_pesos: WeightInitStrategy,

    pub fn init(allocator: std.mem.Allocator,
                configuracion: []const usize,
                funciones_activacion: []const *const fn (f32) f32,
                strat_inicia_pesos: WeightInitStrategy,
                semilla: ?u64) !RedNeuronal {
    }
};
\end{lstlisting}

Esta estructura encapsula todas las capas de la red y gestiona la memoria a través de un allocator. Los parámetros \texttt{configuracion} y \texttt{funciones\_activacion} trabajan en conjunto para definir la arquitectura de la red:

\begin{itemize}
    \item \texttt{configuracion}: Es un array de enteros sin signo que especifica el número de neuronas en cada capa de la red. Por ejemplo, \texttt{[2, 3, 1]} definiría una red con 2 neuronas en la capa de entrada, 3 en la capa oculta, y 1 en la capa de salida.

    \item \texttt{funciones\_activacion}: Es un array de punteros a funciones que define la función de activación para cada capa, exceptuando la capa de entrada. Debe tener una longitud igual a \texttt{configuracion.len - 1}.
\end{itemize}

El primer elemento de \texttt{configuracion} representa la capa de entrada, que no tiene función de activación asociada. Para cada elemento subsiguiente en \texttt{configuracion}, debe haber una función de activación correspondiente en \texttt{funciones\_activacion}. Por ejemplo, si \texttt{configuracion} es \texttt{[2, 3, 1]}, entonces \texttt{funciones\_activacion} debe tener dos elementos: uno para la capa oculta y otro para la capa de salida. Esta correspondencia asegura que cada capa de la red, excepto la de entrada, tenga una función de activación definida.

Esta estructura permite una gran flexibilidad en el diseño de la red, permitiendo configurar fácilmente el número de capas, el número de neuronas en cada capa, y la función de activación específica para cada capa.

El parámetro \texttt{strat\_inicia\_pesos} permite elegir la estrategia de inicialización de pesos para la red, crucial para el rendimiento y la convergencia durante el entrenamiento. Se implementaron dos estrategias:

\begin{itemize}
    \item \textbf{Constante}: Inicializa todos los pesos con un valor constante predefinido.
    \item \textbf{Aleatoria uniforme}: Inicializa los pesos con valores aleatorios de una distribución uniforme.
\end{itemize}
El parámetro \texttt{semilla} permite la reproducibilidad de los experimentos cuando se utiliza la inicialización aleatoria.

Esta estructura proporciona una interfaz completa para la creación y uso de redes neuronales simples, con métodos para inicializar la red, propagar las entradas hacia adelante, y realizar el entrenamiento.

\subsubsection*{Propagación Hacia Adelante}
Se implementó la propagación hacia adelante como un método de la estructura \texttt{RedNeuronal} para calcular la salida de la red:

\begin{lstlisting}[style=customcode]
pub fn propagar_adelante(self: *const RedNeuronal, entradas: []const f32) ![]f32 {
}
\end{lstlisting}

Este método toma las entradas, las procesa a través de cada capa de la red, y devuelve la salida final.

\subsubsection*{Entrenamiento Simplificado}
Desarrollamos un método de entrenamiento básico sin retropropagación compleja. Este enfoque simplificado ajusta los pesos y sesgos de la red basándose en la diferencia entre la salida predicha y la salida deseada. El proceso de entrenamiento sigue estos pasos:

\begin{enumerate}
    \item \textbf{Propagación hacia adelante:} Para cada conjunto de entradas de entrenamiento, la red calcula su salida actual utilizando la función \texttt{propagar\_adelante}.

    \item \textbf{Cálculo del error:} Se calcula la diferencia entre la salida predicha y la salida objetivo utilizando el Error Cuadrático Medio (Mean Squared Error, MSE). El MSE es una métrica común en aprendizaje automático que mide el promedio de los errores al cuadrado. Se calcula como:
    
    \[
    MSE = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
    \]
    
    Donde $n$ es el número de muestras, $Y_i$ es el valor real y $\hat{Y}_i$ es el valor predicho por la red. El MSE penaliza errores más grandes de manera más severa debido al término cuadrático, lo que lo hace útil para muchos problemas de regresión. Este error se utiliza para ajustar los pesos y sesgos de la red.
    
    \item \textbf{Ajuste de pesos y sesgos:} Para cada neurona en cada capa, comenzando desde la capa de salida y moviéndose hacia atrás:
    \begin{itemize}
        \item Los pesos se ajustan en proporción al error, la entrada correspondiente y la tasa de aprendizaje.
        \item El sesgo se ajusta en proporción al error y la tasa de aprendizaje.
    \end{itemize}

    \item \textbf{Iteración:} Los pasos 1-3 se repiten para cada ejemplo en el conjunto de entrenamiento, y todo el proceso se repite durante un número especificado de épocas.
\end{enumerate}

La función de entrenamiento tiene la siguiente firma:
\begin{lstlisting}[style=customcode]
fn entrenar_simple(red: *RedNeuronal, entradas: []const []const f32, 
                   objetivos: []const []const f32, epocas: usize, 
                   tasa_aprendizaje: f32) !void
\end{lstlisting}

Donde:
\begin{itemize}
    \item \texttt{red} es un puntero a la red neuronal a entrenar.
    \item \texttt{entradas} es un array de arrays que contiene los datos de entrada.
    \item \texttt{objetivos} es un array de arrays que contiene las salidas deseadas correspondientes.
    \item \texttt{epocas} es el número de veces que se iterará sobre todo el conjunto de datos.
    \item \texttt{tasa\_aprendizaje} controla cuánto se ajustan los pesos en cada iteración.
\end{itemize}

Este método de entrenamiento permite a la red aprender patrones simples en los datos. Sin embargo, tiene limitaciones en comparación con métodos más avanzados como la retropropagación. Es menos eficiente para redes profundas con múltiples capas ocultas y puede tener dificultades para aprender relaciones no lineales complejas. Además, la convergencia tiende a ser más lenta y menos estable que con algoritmos más sofisticados. A pesar de estas limitaciones, este enfoque simplificado proporciona una base sólida para entender los principios fundamentales del entrenamiento de redes neuronales y puede ser efectivo para problemas simples o como punto de partida para implementaciones más avanzadas.

\section{Resultados}
Hemos logrado implementar con éxito una red neuronal simple pero funcional en el lenguaje de programación Zig. Nuestros resultados principales son:

\begin{itemize}
    \item \textbf{Arquitectura básica:} Implementamos estructuras de datos para neuronas, capas y la red neuronal completa, permitiendo la creación de redes con múltiples capas y número variable de neuronas por capa.

    \item \textbf{Propagación hacia adelante:} Desarrollamos un mecanismo de propagación hacia adelante que permite a la red procesar entradas y generar salidas.

    \item \textbf{Entrenamiento simplificado:} Implementamos un método de entrenamiento básico que permite a la red ajustar sus pesos y sesgos para aprender patrones en los datos.

    \item \textbf{Aprendizaje de funciones lineales:} Demostramos que nuestra red es capaz de aprender funciones lineales simples, validando así su funcionalidad básica.

    \item \textbf{Flexibilidad en la inicialización:} Implementamos diferentes estrategias de inicialización de pesos, incluyendo inicialización constante y aleatoria uniforme.

    \item \textbf{Gestión de memoria eficiente:} Utilizamos las capacidades de Zig para una gestión de memoria eficiente y segura en toda la implementación.
\end{itemize}

Estos resultados demuestran que hemos logrado nuestro objetivo de implementar una red neuronal funcional desde cero en Zig. La red es capaz de realizar tareas básicas de aprendizaje automático, sirviendo como una base sólida para futuros desarrollos y mejoras.

Sin embargo, es importante reconocer las áreas de mejora potencial:
\begin{itemize}
    \item Implementación de algoritmos de entrenamiento más avanzados, como la retropropagación completa.
    \item Adición de más funciones de activación para manejar relaciones no lineales.
    \item Incorporación de técnicas de regularización para mejorar la generalización.
    \item Optimización del rendimiento, posiblemente mediante el uso de operaciones vectoriales.
\end{itemize}

Estas áreas de mejora representan direcciones interesantes para el desarrollo futuro de este proyecto.

\begin{thebibliography}{9}
\bibitem{goodfellow2016deep} 
Goodfellow, I., Bengio, Y., \& Courville, A. (2016). 
\textit{Deep Learning}. 
MIT Press.

\bibitem{zigdoc}
Zig Software Foundation. (2023).
\textit{Zig Programming Language Documentation}.
\url{https://ziglang.org/documentation/master/}

\bibitem{kinsley2020neural}
Kinsley, H., \& Kukieła, D. (2020).
\textit{Neural Networks from Scratch in Python}.
Self-published.

\bibitem{howard2020deep}
Howard, J., \& Gugger, S. (2020).
\textit{Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD}.
O'Reilly Media.
\end{thebibliography}

\end{document}